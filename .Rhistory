install.packages("caret")
library(caret)
install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
head(read.csv(filename=iris.csv, header=FALSE))
head(read.csv(filename="iris.csv", header=FALSE))
head(read.csv("iris.csv", header=FALSE))
read.csv("iris.csv", header=FALSE)
filename <- "iris.csv"
read.csv(filename, header=FALSE)
rm(filename)
# Load Data
load(iris)
# Load Data
data(iris)
# Load Data
load(iris)
# Load Data
data(iris)
head(iris)
data(oil)
dim(oil)
head(oil)
data(oil)
head(oil)
oil
oilType
dim(oilType)
## Load Data
data(iris)
dataset <- iris # rename the dataset
## Create a validation set
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index, ]
# use the remaning 80% of dta to training and testing the models
dataset <- dataset[validation_index, ]
validation_index
dim(iris)
validation
dataset
## 3. Summarize Dataset
dim(dataset)
sapply(dataset, class) # list types for each attributes
head(dataset) # peek at the data
levels(dataset$Species) # list the levels of the class
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
percentage
m <- matrix(1:4, 2)
m
prop.table(m, 1)
m <- matrix(c(1,2,2,4), 2)
m
prop.table(m, 1)
table(dataset$Species)
summary(dataset)
summary(dataset)
## 4. Visualize dataset
# Unvariate plots (To better understand each attribute)
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
# boxplot for each attribute on one image
par(mfrow=c(1,4))
# boxplot for each attribute on one image
par(mfrow=c(1,4))
# boxplot for each attribute on one image
par(mfrow=c(1,4))
for (i in 1:4) {
boxplot(x[,i], main=names(iris)[i])
}
par(mfrow=c(1,4))
for (i in 1:4) {
boxplot(x[,i], main=names(iris)[i])
}
plot(y)
# boxplot for each attribute on one image
par(mfrow=c(1,4))
for (i in 1:4) {
boxplot(x[,i], main=names(iris)[i])
}
plot(y)
# Multivaraite Plots
featurePlot(x=x, y=y, plot = "ellipse") # scatterplot matrix
featurePlot(x=x, y=y, plot = "box") # # box and whisker plots for each attribute
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot = "density", scales=scales)
## 3. Summarize Dataset
dim(dataset) # dimension of dataset
sapply(dataset, class) # list types for each attributes
head(dataset) # peek at the data
levels(dataset$Species) # list the levels of the class
# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)
prop.table(table(dataset$Species))
prop.table(table(dataset$Species)) * 100
table(dataset$Species)
# statistical summary
summary(dataset)
## 4. Visualize dataset
# Unvariate plots (To better understand each attribute)
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
# boxplot for each attribute on one image
par(mfrow=c(1,4))
for (i in 1:4) {
boxplot(x[,i], main=names(iris)[i])
}
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot = "density", scales=scales)
library(caret)
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot = "density", scales=scales)
## 5. Evaluate Some Algorithms
# Run algortihm using 10-fold cross validation
control <- trainControl(method = "cv", number = 10)
metric <- "Accuracy"
control
set.seed(7)
set.seed(7)
a
Species~.
## 5. Evaluate Some Algorithms
# Run algortihm using 10-fold cross validation
control <- trainControl(method = "cv", number = 10)
metric <- "Accuracy"
# Build Models
# a. linear algorithm
set.seed(7)
fit.lda <- train(Species~., data = dataset, method = "lda", metric=metric, trControl=control)
# b. nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(Species~., data = dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=mertric, trControl=control)
# c. advanced algorithm
# SVM
set.seed(7)
fit.svm <- train(Species~., data = dataset, method="snmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(Species~., data = dataset, method="rf", metric=metric, trControl=control)
fit.lada
fit.lda
fit.svm
# c. advanced algorithm
# SVM
set.seed(7)
fit.svm <- train(Species~., data = dataset, method="snmRadial", metric=metric, trControl=control)
fit.svm <- train(Species~., data = dataset, method="svmRadial", metric=metric, trControl=control)
# c. advanced algorithm
# SVM
set.seed(7)
fit.svm <- train(Species~., data = dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(Species~., data = dataset, method="rf", metric=metric, trControl=control)
fit.svm
fit.lda
fit.svm
## Select best model
# summarize accuracy of models
results <- resamples(list(lad=fit.lda,  cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
# kNN
set.seed(7)
fit.knn <- train(Species~., data=dataset, method="knn", metric=mertric, trControl=control)
fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control)
## Select best model
# summarize accuracy of models
results <- resamples(list(lad=fit.lda,  cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
## Select best model
# summarize accuracy of models
results <- resamples(list(lda=fit.lda,  cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
results
View(results)
View(results)
results$values
results$metrics
results$models
results$call
results
summary(results)
plot(results)
boxplot(results)
# summarize best model
print(fit.lda)
# summarize best model
print(fit.knn)
# summarize best model
print(fit.svm)
# summarize best model
print(fit.rf)
## 6. Make Predictions
# estimate skill of LDA on the validation dataset
prediction <- predict(fit.lda, validation)
prediction
confusionMatrix(prediction, validation$Species)
predict
confusionMatrix(prediction, validation$Species)
